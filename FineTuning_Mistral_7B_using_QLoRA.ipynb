{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6vfwwCkwUU1"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhqisWYgnpZr"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "# !pip install -q -U pandas # you don't need to install either of these last two libs if you're using Colab\n",
    "# !pip install -q -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "oiA_BhEjLu9w",
    "outputId": "6089afb5-733a-4a27-a5c4-46197b38f2ab"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "\n",
    "!pip uninstall Jinja2 -y # Uninstall any existing Jinja2\n",
    "!pip install Jinja2==3.0.3 # Or another compatible version like 3.0.3 or 3.0.x\n",
    "!pip install Jinja2==3.0.3 # Or another compatible version like 3.0.3 or 3.0.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN2yH0ldNMBK"
   },
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAJoMqj3B_y8"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import peft\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arHQdKHHbNzn"
   },
   "outputs": [],
   "source": [
    "mistral7b = 'mistralai/Mistral-7B-v0.1'\n",
    "model_name = mistral7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "784wEoZ3XfKt"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "id": "J5dsWhGCB_zF",
    "outputId": "b1d4a6b4-e827-4632-8f47-cc14e969ef5f"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"frankenstein_chunks.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "yeZZ59rEZxvY",
    "outputId": "58c335e8-6235-494c-d418-738f732a9339"
   },
   "outputs": [],
   "source": [
    "print(\"Dataframe Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "print(\"Dataframe Description:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "print(\"Number of unique values in each column:\")\n",
    "print(df.nunique())\n",
    "random_index= random.randint(0, len(df) - 1)\n",
    "df.loc[random_index, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukR-7-wiZC1v",
    "outputId": "647fe763-4d0b-4a97-ea04-38ac15859c2b"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYkwTOqQlHNd"
   },
   "outputs": [],
   "source": [
    "# Now we'll quickly convert this to a train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# STEP 2. Convert the train_df and test_df from Pandas into Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kObQJA4sXdIq"
   },
   "source": [
    "## Model Import and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKp99ZZQLJDI"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(os.getenv(\"HF_TOKEN\"))  # Read from environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942,
     "referenced_widgets": [
      "6ba9478ffeb54836adaeaf58c30bd1ed",
      "6fbe3fbbf109440fb7398ba0745cc6d9",
      "a2c4633cb2d14f61aa0d3baa2bcdb17a",
      "bf5304ca111e4e808f8d7c26c3072c08",
      "445641e95489476588f6bd70649cbf0d",
      "fd84a42814d8432c9d27f76e6325bd28",
      "8bf65670188d41048a8ea305b9bfcf7a",
      "6e128054dbfc496abd8b8158f90c5dfe",
      "7bc509eefcbf45ca9d92359843261219",
      "cd39d43ce632446aaa5715ccf6768306",
      "c330a3eab4af4ed3bfdbd89fd47603fe",
      "b48e7ae042354b8d9522e2cb831123c6",
      "38f03e9b0ec24e42bd1245e1b5964b2e",
      "c4f3f324baa1402e882110387b7d53d4",
      "3cc41cde1ac242ae95d41d25daa40a70",
      "457efc081cfb491dad61886872ec2025",
      "6cdfa71037b94e1a848b6749aabf77f5",
      "b72e6f0f0b784678904e770d8b889d55",
      "37c698c276a344c8921ca32adfbf8887",
      "6606d11fd98a48e3aeb6902b4a173df5",
      "48f47906b69946a98c0c69ff4c7e1d52",
      "e8a10ca441834877a01178cbb13323c7",
      "8103a2feae444fdbb08c83f781017b09",
      "0e812092079d418d984d69c5b928480c",
      "b14d44666f354144ade2ecd30a9e5255",
      "81865241d9ac4d09b4aefae5976600ac",
      "215f6e8023064c9ebd6908c40fa2a1de",
      "f0f33c4f4aff402a81ddb4c844766d6b",
      "cc44b3ce37f34ae899d459bf3c55c8a8",
      "7e4248eebc02468e91bbf560d67ffbcc",
      "0ba9227a51044e06b4767a79461b3170",
      "d8612f13df894445ae6abd8c75eaba58",
      "1fddccfa48274b6b94b72ccf18fb7bc5",
      "ee28681355af4a038701438b0506d3c3",
      "6922a180b90f4d24b3b7544e40f119ba",
      "764cf542b61748c8b43406bcdd026234",
      "f31f85e0fd2244be94fdcb63cf233079",
      "fdff8fef17c348eaab2fa194f40d3b6b",
      "a92f2107eef84b7e9819a21477df9ddd",
      "6ef4a43960a34abaa571f102f9c7cdb4",
      "caf6d0a2ed824ad6a107675df541c7d9",
      "3b333c174a984191b0d4904893dab99f",
      "993f7adcf9d8443cafb9ca72cab7846c",
      "7e5505829ab74b1bb132680a74cdcbd3",
      "b3aacf42482c459496cb9e439c817482",
      "73ff66ddb896426aa1ab8be6bcdb9e21",
      "dfede56e5020468eb018c8e4b3ff9d9f",
      "8b994574cfa64fb9aedea040bd52ec03",
      "baa28778638443c1b5e5a7158cb12515",
      "d044991236174cf796337570f97b79b3",
      "754fcf3f0f9d4f22968f651e34a97614",
      "6e7bffdbf06649079fd1c28fddeeb163",
      "be4131ae7db8460d92b32d1e115acb8d",
      "a69be6e643f9452ba86de5703a720c67",
      "28888d4a0c2240e881d32d2448e0a12e",
      "cffb8c4c169242b6977a2c9324d9c3af",
      "a115ca8f20a846b5920383409c058952",
      "df2e5db8f56741c5b6ed923c35fa9611",
      "4695023e08d64d80a2f8dcabe53af732",
      "d4c342a427134f89bab2f6eeb086046b",
      "cc50d618a196425a856c3ea63dcfd976",
      "e46871d920db405fa3234dcf4aeb3661",
      "4fb12848456149d3b72bbc17d40f0246",
      "310b47f660be439b872a49d7aa846379",
      "2e195f3e1c03487eaa354b7fd05e3288",
      "84791a37dc87489ab7ccad5cd30307ea",
      "00718ca22b6f48f98f0e0bf1fb71b2a5",
      "00e97cc8bead46b286532ff41a6e1540",
      "9e180b700d584e6b9abf44ecf975fb1a",
      "44059c8c268242e6a81f04b5e0a7a3c1",
      "cb6b94a375bf45d898aeabecaffcbcb5",
      "aa031eda79734ccdbddf85032440e812",
      "b36e353e3b4640f99b3f82ce26cc96a2",
      "c44810848f01484a9ab6755c91ec5816",
      "7a2f097fcd594d9fbd901403f240c3d3",
      "6d0e5df6597842579df8e3dad4e3120e",
      "03eea08ee57f401b89339ac19c58a365"
     ]
    },
    "id": "xFg2ZsKupHBE",
    "outputId": "3a29bb00-73e4-4cfd-eaeb-b07ad42bc6f8"
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "  # STEP 3. Passing the appropriate parameters here to 4-bit quantize the model\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
    "print(\"\\n\\nModel is running on:\" + \"\\n\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "a03a29317f7042e59c37911bb505e6d5",
      "5f1e1514c40c4dc7bb3dfaa193be7a99",
      "5c0f9c97d2424c2d8f5356ce680d1459",
      "c4894d0f8922473abba0ed1be54c4a71",
      "00340fe0008a49a590a6abf0acaf5013",
      "ab3ee278936049ae8946b77c0e44e1bf",
      "96e4821fd56649949f534c9039f57e43",
      "73e93c7d183f40809d09137983739c39",
      "c2dbd741cd5d4ec69e99544e77dc76ff",
      "7030d40eceb244a4bd4bb62e7ebabfb0",
      "9d52194841044e388a92236aa5c8279b",
      "27e0fbbdb9b94a45a82a0592e0c48733",
      "7958be1157af4484898a06de9b4dda9f",
      "56d3f51bba014dc2b064173e1f4b0304",
      "cd912a1ab36f4ad48410453d5ed4b85d",
      "2c972978ad8c4327b0d245a02d0905c8",
      "b1d58ff583d340d69774f2ae22925995",
      "0928e9bb8b924992a799e86b32046edc",
      "af5df260b4314d418e03858d45115da7",
      "185e5d64e1954770831edcbbd9afd44b",
      "1318dc7a79be4f919484cfff771a5e7d",
      "28ef587687e54421b32cb4315c853338",
      "bbaf92aed9cf4fe8a179fe67d190af8d",
      "7511097ba79245fe98090adac54bc2b8",
      "35515d9c5cd84ff5a4cc07da0c4e61e4",
      "0de5541170fe4f2883865bdda9354874",
      "7c40075645a04358b77a8b20f7a55b7e",
      "dc35a5cef0f24991af18dd559d3c137f",
      "5063af1aa2524c7ab7a4e4d7e834896d",
      "70fec48b57944b1fae7aa785502ad167",
      "295b27cde5f04399b53145aba3274e57",
      "62f688ed097340a1a74bf0312b863086",
      "6d1980d7e4bf419eb77cd72a6371ca9c"
     ]
    },
    "id": "O6kwn8qlpwnU",
    "outputId": "28b65133-db42-4776-f64c-b87bee2ba776"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# STEP 4. Preparing the model for QLoRA. Configure LoRA for our finetuning run. Then tokenize the data.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_train_dataset=train_dataset.map(lambda examples: tokenizer(examples['text'], padding=\"longest\", truncation=True), batched=True)\n",
    "tokenized_test_dataset =test_dataset.map(lambda examples: tokenizer(examples['text'], padding=\"longest\", truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wojUYNXdHRD"
   },
   "source": [
    "## Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-h2SUXAILft8"
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "  device = \"cuda\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "  outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "  output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "sRyYRTVICSVf",
    "outputId": "ec4ded5c-06eb-490a-cb75-0eb460a31a04"
   },
   "outputs": [],
   "source": [
    "# STEP 5. Generating a completion with the base model for informal evaluation.\n",
    "base_generation =generate_text(\"I'm afraid I've created a \")\n",
    "base_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwwvV0_xdJHf",
    "outputId": "a4427804-9121-4613-f6ca-55d4fa4ddab3"
   },
   "outputs": [],
   "source": [
    "def calc_perplexity(model):\n",
    "  total_perplexity = 0\n",
    "  # Determine the device the model is on\n",
    "  device = next(model.parameters()).device\n",
    "  for row in test_dataset:\n",
    "    inputs = tokenizer(row['text'], return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Move the input tensors to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Calculate the loss without updating the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    # STEP 6. Complete the equation for perplexity.\n",
    "    perplexity = torch.exp(torch.tensor(loss))\n",
    "    total_perplexity += perplexity\n",
    "\n",
    "  num_test_rows = len(test_dataset)\n",
    "  avg_perplexity = total_perplexity / num_test_rows\n",
    "  return avg_perplexity\n",
    "\n",
    "base_ppl = calc_perplexity(model)\n",
    "base_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkZFOmpStML4"
   },
   "source": [
    "## Training\n",
    "\n",
    "Make sure you can leave your browser open for a while. This may take around 15-25 minutes on a Colab T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TlykrY27qThu",
    "outputId": "c972e0b2-031c-4622-cb4e-843f0f28cc18"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        warmup_steps=2,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        save_steps=200,\n",
    "        output_dir=\"outputs\",\n",
    "      # STEP 7. Configure the training arguments.\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=0.00002,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "# STEP 8. Finetuning the model.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuIyfyYJq4yB"
   },
   "source": [
    "## Evaluating the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JG6D3Zm2cBEa",
    "outputId": "b54b110f-85f6-464b-80b9-d8a36717a662"
   },
   "outputs": [],
   "source": [
    "# STEP 9. Generating a completion with the finetuned model and compare it to the base generation.\n",
    "ft_generation = generate_text(\"I'm afraid I've created a \")\n",
    "\n",
    "print(\"Base model generation: \" + base_generation + \"\\n\\n\")\n",
    "print(\"Finetuned generation: \" + ft_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G051yZQGuOK"
   },
   "source": [
    "A little more like the original text, right?\n",
    "Experimenting with the hyperparameters to see if you can improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSngfXmyl2C1",
    "outputId": "5107b9e4-2ba4-439e-cdc3-03316c25f155"
   },
   "outputs": [],
   "source": [
    "# STEP 10. Calculating  the finetuned model's perplexity and compare it to the base model's.\n",
    "ft_ppl = calc_perplexity(model)\n",
    "print(\"Base model perplexity: \" + str(base_ppl))\n",
    "print(\"Finetuned model perplexity: \" + str(ft_ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GurNWs0b1ojU",
    "outputId": "ad9f5240-ca5a-4f15-d3f1-1d744d6e7a3f"
   },
   "outputs": [],
   "source": [
    "# Calculates and prints the total number of parameters and the number of trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
